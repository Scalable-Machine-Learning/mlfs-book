{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16b7819",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"> **Air Quality** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 04: Batch Inference</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Download model and batch inference data\n",
    "2. Make predictions, generate PNG for forecast\n",
    "3. Store predictions in a monitoring feature group adn generate PNG for hindcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a84ee9",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb904bb-a8c4-45d7-b54e-6d24f689c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Added the following directory to the PYTHONPATH: /Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book\n",
      "HopsworksSettings initialized!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "    \n",
    "# Read the API keys and configuration variables from the file <root_dir>/.env\n",
    "from mlfs import config\n",
    "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f430c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import hopsworks\n",
    "import json\n",
    "from mlfs.airquality import util\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a4265d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 11, 18, 15, 15, 39, 16748)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = datetime.datetime.now() - datetime.timedelta(0)\n",
    "tomorrow = today + datetime.timedelta(days = 1)\n",
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91e99d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a2c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-18 15:15:39,026 INFO: Initializing external client\n",
      "2025-11-18 15:15:39,026 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-11-18 15:15:39,626 WARNING: UserWarning: The installed hopsworks client version 4.4.2 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-18 15:15:40,527 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1271977\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login(engine=\"python\", project=\"air_quality_prediction\")\n",
    "fs = project.get_feature_store() \n",
    "\n",
    "secrets = hopsworks.get_secrets_api()\n",
    "# location_str = secrets.get_secret(\"SENSOR_LOCATION_JSON\").value\n",
    "# location = json.loads(location_str)\n",
    "# country=location['country']\n",
    "# city=location['city']\n",
    "# street=location['street']\n",
    "\n",
    "sensor_secret_names = [\n",
    "    \"SENSOR_LOCATION_bankgatan_JSON\",\n",
    "    \"SENSOR_LOCATION_linakersvagen_JSON\",\n",
    "    \"SENSOR_LOCATION_trollebergsvagen_JSON\",\n",
    "]\n",
    "\n",
    "sensors = {}\n",
    "for name in sensor_secret_names:\n",
    "    data = json.loads(secrets.get_secret(name).value)\n",
    "    sensors[data[\"street\"]] = {\n",
    "        \"country\":   data[\"country\"],\n",
    "        \"city\":      data[\"city\"],\n",
    "        \"street\":    data[\"street\"],\n",
    "        \"latitude\":  data[\"latitude\"],\n",
    "        \"longitude\": data[\"longitude\"],\n",
    "        \"aqicn_url\": data[\"aqicn_url\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb1c037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.92s) \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "air_quality_fg = fs.get_feature_group(\n",
    "    name='air_quality',\n",
    "    version=1\n",
    ")\n",
    "\n",
    "air_quality_df_full = air_quality_fg.read()\n",
    "streets_sorted = sorted(air_quality_df_full['street'].dropna().unique().tolist())\n",
    "street_encoder = LabelEncoder().fit(streets_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cead441",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™ù Download the model from Model Registry</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d70a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-18 15:15:48,633 INFO: Initializing for batch retrieval of feature vectors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b093fd7f7e490bbb9080e68d2dd241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/106511 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4783752eef654092b2c733d0ddfabe70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/119756 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 2 files)... \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f3963a3a6547698482ad897409772c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/106678 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 3 files)... \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dca0a48c3c94e9d913b7fabe7ae877f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/116397 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 4 files)... \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da9619e800844e193fecbd9c36900f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/26776 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (1 dirs, 5 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "retrieved_model = mr.get_model(\n",
    "    name=\"air_quality_xgboost_model\",\n",
    "    version=3,\n",
    ")\n",
    "\n",
    "fv = retrieved_model.get_feature_view()\n",
    "\n",
    "# Download the saved model artifacts to a local directory\n",
    "saved_model_dir = retrieved_model.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6cf6c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=&#x27;8.547116E0&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;,\n",
       "                            &#x27;float&#x27;, &#x27;float&#x27;, &#x27;int&#x27;],\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=&#x27;8.547116E0&#x27;, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=[&#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;, &#x27;float&#x27;,\n",
       "                            &#x27;float&#x27;, &#x27;float&#x27;, &#x27;int&#x27;],\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score='8.547116E0', booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None,\n",
       "             feature_types=['float', 'float', 'float', 'float', 'float',\n",
       "                            'float', 'float', 'int'],\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the XGBoost regressor model and label encoder from the saved model directory\n",
    "# retrieved_xgboost_model = joblib.load(saved_model_dir + \"/xgboost_regressor.pkl\")\n",
    "retrieved_xgboost_model = XGBRegressor()\n",
    "\n",
    "retrieved_xgboost_model.load_model(saved_model_dir + \"/model.json\")\n",
    "\n",
    "# Displaying the retrieved XGBoost regressor model\n",
    "retrieved_xgboost_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad941a",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚ú® Get Weather Forecast Features with Feature View   </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaacae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e4491",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">ü§ñ Making the predictions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebeaa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hsfs.feature_group.FeatureGroup at 0x139ba37c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "today = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "monitor_fg = fs.get_or_create_feature_group(\n",
    "    name='aq_predictions_clean',\n",
    "    description='Air Quality prediction monitoring',\n",
    "    version=1,\n",
    "    primary_key=['city','street','date','days_before_forecast_day'],\n",
    "    event_time=\"date\",\n",
    ")\n",
    "\n",
    "monitor_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bfb9f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.37s) \n",
      "2025-11-18 15:15:55,509 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2025-11-18 15:15:55,510 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.86s) \n",
      "Using 3 lagged historical feature.\n",
      "Using 2 lagged historical feature.\n",
      "Using 1 lagged historical feature.\n",
      "Using only predicted data as lagged feature.\n",
      "Using only predicted data as lagged feature.\n",
      "Using only predicted data as lagged feature.\n",
      "Using only predicted data as lagged feature.\n",
      "2025-11-18 15:15:57,867 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2025-11-18 15:15:57,868 WARNING: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "\n",
      "2025-11-18 15:15:58,884 ERROR: Parser Error: . Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 75, in read_query\n",
      "    with duckdb.connect(\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 101, in read_query\n",
      "    return duckdb_con.execute(query_string).arrow(self.query_result_batch_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "duckdb.duckdb.ParserException: Parser Error: \n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: Parser Error: \n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"Parser Error: . Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 75, in read_query\\n    with duckdb.connect(\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 101, in read_query\\n    return duckdb_con.execute(query_string).arrow(self.query_result_batch_size)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nduckdb.duckdb.ParserException: Parser Error: \\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: Parser Error: \\n\", grpc_status:2, created_time:\"2025-11-18T15:15:58.883371+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/core/arrow_flight_client.py\", line 395, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"/Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/core/arrow_flight_client.py\", line 460, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"/Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"/Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"/Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"/Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/core/arrow_flight_client.py\", line 446, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow/_flight.pyx\", line 1745, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow/_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: Parser Error: . Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 75, in read_query\n",
      "    with duckdb.connect(\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 101, in read_query\n",
      "    return duckdb_con.execute(query_string).arrow(self.query_result_batch_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "duckdb.duckdb.ParserException: Parser Error: \n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: Parser Error: \n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"Parser Error: . Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 75, in read_query\\n    with duckdb.connect(\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 101, in read_query\\n    return duckdb_con.execute(query_string).arrow(self.query_result_batch_size)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nduckdb.duckdb.ParserException: Parser Error: \\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: Parser Error: \\n\", grpc_status:2, created_time:\"2025-11-18T15:15:58.883371+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Could not read data using Hopsworks Query Service.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFlightServerError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/core/arrow_flight_client.py:395\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/core/arrow_flight_client.py:460\u001b[0m, in \u001b[0;36mArrowFlightClient.read_query\u001b[0;34m(self, query_object, arrow_flight_config, dataframe_type)\u001b[0m\n\u001b[1;32m    459\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightDescriptor\u001b[38;5;241m.\u001b[39mfor_command(query_encoded)\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrow_flight_config\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py:55\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrying\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py:279\u001b[0m, in \u001b[0;36mRetrying.call\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reject(attempt):\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mattempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mwarning(attempt)\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py:326\u001b[0m, in \u001b[0;36mAttempt.get\u001b[0;34m(self, wrap_exception)\u001b[0m\n\u001b[1;32m    325\u001b[0m         exc_type, exc, tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/retrying.py:273\u001b[0m, in \u001b[0;36mRetrying.call\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m Attempt(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/core/arrow_flight_client.py:446\u001b[0m, in \u001b[0;36mArrowFlightClient._get_dataset\u001b[0;34m(self, descriptor, timeout, dataframe_type)\u001b[0m\n\u001b[1;32m    443\u001b[0m options \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightCallOptions(\n\u001b[1;32m    444\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_certificates_headers()\n\u001b[1;32m    445\u001b[0m )\n\u001b[0;32m--> 446\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mticket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset fetched. Converting to dataframe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe_type)\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/pyarrow/_flight.pyx:1745\u001b[0m, in \u001b[0;36mpyarrow._flight.FlightClient.do_get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/pyarrow/_flight.pyx:58\u001b[0m, in \u001b[0;36mpyarrow._flight.check_flight_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFlightServerError\u001b[0m: Parser Error: . Detail: Python exception: Traceback (most recent call last):\n  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n    result = func(instance, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 196, in do_get\n    return self._read_query(context, path, command)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n    return func(instance, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n    result_batches = self.hudi_query_engine.read_query(query_obj)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_query_engine.py\", line 75, in read_query\n    with duckdb.connect(\n  File \"/usr/src/app/src/hudi_query_engine.py\", line 101, in read_query\n    return duckdb_con.execute(query_string).arrow(self.query_result_batch_size)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nduckdb.duckdb.ParserException: Parser Error: \n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n    raise FlyingDuckException(str(e)) from e\nutils.exceptions.FlyingDuckException: Parser Error: \n. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"Parser Error: . Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 75, in read_query\\n    with duckdb.connect(\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 101, in read_query\\n    return duckdb_con.execute(query_string).arrow(self.query_result_batch_size)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nduckdb.duckdb.ParserException: Parser Error: \\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: Parser Error: \\n\", grpc_status:2, created_time:\"2025-11-18T15:15:58.883371+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m batch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet_encode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m street_encoder\u001b[38;5;241m.\u001b[39mtransform(batch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# doing predictions for specific street. This allows for error raise if e.g. one sensor has no data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m hist_aq \u001b[38;5;241m=\u001b[39m \u001b[43mair_quality_fg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mair_quality_fg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstreet\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mair_quality_fg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoday\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hist_aq\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] No historical air quality for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstreet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; skipping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/constructor/query.py:309\u001b[0m, in \u001b[0;36mQuery.read\u001b[0;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoins) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m schema]:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         )\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/engine/python.py:146\u001b[0m, in \u001b[0;36mEngine.sql\u001b[0;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msql\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     sql_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     schema: Optional[List[feature\u001b[38;5;241m.\u001b[39mFeature]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    144\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, pl\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_offline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow_flight_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdbc(\n\u001b[1;32m    156\u001b[0m             sql_query, online_conn, dataframe_type, read_options, schema\n\u001b[1;32m    157\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/engine/python.py:189\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[0;34m(self, sql_query, dataframe_type, schema, arrow_flight_config)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql_query, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_string\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sql_query:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhsfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m arrow_flight_client\n\u001b[0;32m--> 189\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_loading_animation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReading data from Hopsworks, using Hopsworks Feature Query Service\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading data with Hive is not supported when using hopsworks client version >= 4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hopsworks_common/util.py:373\u001b[0m, in \u001b[0;36mrun_with_loading_animation\u001b[0;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/KTH/Year 1/ID2223-SML/mlfs-book/aq/lib/python3.10/site-packages/hsfs/core/arrow_flight_client.py:413\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[0;34m(instance, *args, **kw)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: Could not read data using Hopsworks Query Service."
     ]
    }
   ],
   "source": [
    "all_preds_for_city = []  \n",
    "\n",
    "weather_base = weather_fg.filter(weather_fg.date >= today).read().copy()\n",
    "weather_base['date'] = pd.to_datetime(weather_base['date']).dt.tz_localize(None)\n",
    "weather_base = weather_base.sort_values('date').reset_index(drop=True)\n",
    "weather_base['days_before_forecast_day'] = ((weather_base['date'] - today).dt.days)\n",
    "weather_base = weather_base.query('days_before_forecast_day >= 1')\n",
    "\n",
    "for street, meta in sensors.items():\n",
    "\n",
    "    batch_data = weather_base.copy()\n",
    "    batch_data['country'] = meta['country']\n",
    "    batch_data['city']    = meta['city']\n",
    "    batch_data['street']  = meta['street']\n",
    "\n",
    "    batch_data['street_encode'] = street_encoder.transform(batch_data['street']).astype('float32')\n",
    "\n",
    "    # doing predictions for specific street. This allows for error raise if e.g. one sensor has no data\n",
    "    hist_aq = air_quality_fg.filter(\n",
    "        (air_quality_fg.street == street) & \n",
    "        (air_quality_fg.date < today)\n",
    "    ).read()\n",
    "    \n",
    "    if hist_aq.empty:\n",
    "        print(f\"[WARN] No historical air quality for {street}; skipping.\")\n",
    "        continue\n",
    "    \n",
    "    hist_aq['date'] = pd.to_datetime(hist_aq['date']).dt.tz_localize(None)\n",
    "    hist_aq = hist_aq.sort_values('date').tail(3).reset_index(drop=True) #only keep the last three days\n",
    "    \n",
    "    if len(hist_aq) < 3:\n",
    "        print(f\"[WARN] Less than 3 days of history for {street}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    for idx, row in batch_data.iterrows(): # is in the correct order\n",
    "        day_num = row['days_before_forecast_day']\n",
    "        forecast_idx = idx\n",
    "        \n",
    "        if forecast_idx == 0:\n",
    "            print(\"Using 3 lagged historical feature.\")\n",
    "            lag_1 = hist_aq.iloc[-1]['pm25']\n",
    "            lag_2 = hist_aq.iloc[-2]['pm25']\n",
    "            lag_3 = hist_aq.iloc[-3]['pm25']\n",
    "        elif forecast_idx == 1:\n",
    "            print(\"Using 2 lagged historical feature.\")\n",
    "            lag_1 = predictions[0]\n",
    "            lag_2 = hist_aq.iloc[-1]['pm25']\n",
    "            lag_3 = hist_aq.iloc[-2]['pm25']\n",
    "        elif forecast_idx == 2:\n",
    "            print(\"Using 1 lagged historical feature.\")\n",
    "            lag_1 = predictions[1]\n",
    "            lag_2 = predictions[0]\n",
    "            lag_3 = hist_aq.iloc[-1]['pm25']\n",
    "        else:\n",
    "            print(\"Using only predicted data as lagged feature.\")\n",
    "            lag_1 = predictions[forecast_idx - 1]\n",
    "            lag_2 = predictions[forecast_idx - 2]\n",
    "            lag_3 = predictions[forecast_idx - 3]\n",
    "        \n",
    "        features = [\n",
    "            row['temperature_2m_mean'],\n",
    "            row['precipitation_sum'],\n",
    "            row['wind_speed_10m_max'],\n",
    "            row['wind_direction_10m_dominant'],\n",
    "            row['street_encode'],\n",
    "            lag_1,\n",
    "            lag_2,\n",
    "            lag_3\n",
    "        ]\n",
    "        \n",
    "        pred = retrieved_xgboost_model.predict([features])[0]\n",
    "        predictions.append(pred)\n",
    "\n",
    "    batch_data['predicted_pm25'] = predictions\n",
    "    \n",
    "    to_insert = batch_data.drop(columns=['street_encode']).copy()\n",
    "    to_insert['country'] = to_insert['country'].astype(str)\n",
    "    to_insert['city'] = to_insert['city'].astype(str)\n",
    "    to_insert['street'] = to_insert['street'].astype(str)\n",
    "    to_insert['date'] = pd.to_datetime(to_insert['date']).dt.tz_localize(None)                                                                 \n",
    "    to_insert['days_before_forecast_day'] = to_insert['days_before_forecast_day'].astype('int64')                                                                    \n",
    "\n",
    "    all_preds_for_city.append(to_insert)\n",
    "\n",
    "# upload full dataframe\n",
    "if all_preds_for_city:\n",
    "    final_to_insert = pd.concat(all_preds_for_city).reset_index(drop=True)\n",
    "    #monitor_fg.insert(final_to_insert, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909410b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3871136, 2.360062, 11.490718, 10.554521, 12.044299, 9.002187, 11.92575]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e1488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                           datetime64[us]\n",
       "temperature_2m_mean                   float32\n",
       "precipitation_sum                     float32\n",
       "wind_speed_10m_max                    float32\n",
       "wind_direction_10m_dominant           float32\n",
       "city                                   object\n",
       "country                                object\n",
       "street                                 object\n",
       "days_before_forecast_day                int64\n",
       "street_encode                         float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a3126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timedelta:\n",
      "0    2\n",
      "1    3\n",
      "2    4\n",
      "3    5\n",
      "4    6\n",
      "5    7\n",
      "6    8\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Timedelta:\")\n",
    "print((batch_data['date'] - today).dt.days + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e2b0a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">ü§ñ Saving the predictions (for monitoring) to a Feature Group</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff10c12",
   "metadata": {},
   "source": [
    "### Create Forecast Graph\n",
    "Draw a graph of the predictions with dates as a PNG and save it to the github repo\n",
    "Show it on github pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c99085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "root_dir = str(Path().absolute())\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# this part is specific to me due to directory naming issues. \n",
    "def find_repo_root():\n",
    "    here = Path.cwd()\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"docs\").exists():\n",
    "            return p\n",
    "    return here  \n",
    "\n",
    "project_root = find_repo_root()\n",
    "\n",
    "img_dir = project_root / \"docs\" / \"air-quality\" / \"assets\" / \"img\"\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "today_str = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
    "today_str = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "dataset_api = project.get_dataset_api()\n",
    "if not dataset_api.exists(\"Resources/airquality\"):\n",
    "    dataset_api.mkdir(\"Resources/airquality\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.10s) \n",
      "bankgatan rows: 7 nans: 0 dates: 2025-11-19 00:00:00 ‚Üí 2025-11-25 00:00:00\n",
      "2025-11-18 14:48:37,833 WARNING: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n",
      "\n",
      "[WARN] No overlapping dates for hindcast at 'bankgatan'. Skipping hindcast plot.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c181724d19b47dc8c8f85ab6cb5e932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/docs/air-quality/assets/img/lund_bankgata‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1184a0404f4447ff9b3b9534d8ac2ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /Users/sebastian/Documents/KTH/Year 1/ID2223-SML/mlfs-book/docs/air-quality/assets/img/lund_bankgata‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] No in-memory forecast DF found for street 'lin√•kersv√§gen'. Skipping plots.\n",
      "[WARN] No in-memory forecast DF found for street 'trollebergsv√§gen'. Skipping plots.\n",
      "The images are saved here: https://c.app.hopsworks.ai:443/p/1271977/settings/fb/path/Resources/airquality\n"
     ]
    }
   ],
   "source": [
    "city = next(iter(sensors.values()))['city']  # since all sensors are in the same city\n",
    "preds_1day_city = monitor_fg.filter(\n",
    "    (monitor_fg.city == city) &\n",
    "    (monitor_fg.days_before_forecast_day == 1)\n",
    ").read().copy()\n",
    "\n",
    "def _norm_dates(series):\n",
    "    s = pd.to_datetime(series, errors=\"coerce\")\n",
    "    try:\n",
    "        s = s.dt.tz_convert(None)   \n",
    "    except Exception:\n",
    "        pass\n",
    "    return s.dt.normalize()\n",
    "\n",
    "for street, meta in sensors.items():\n",
    "\n",
    "    preds_1day = preds_1day_city[preds_1day_city['street'] == street].copy()\n",
    "\n",
    "    outcomes = air_quality_df_full[\n",
    "        (air_quality_df_full['city'] == meta['city']) &\n",
    "        (air_quality_df_full['street'] == street)\n",
    "    ][['date', 'pm25']].copy()\n",
    "\n",
    "\n",
    "    try:\n",
    "        nday_df = next(df for df in all_preds_for_city if df['street'].iloc[0] == street)\n",
    "    except StopIteration:\n",
    "        print(f\"[WARN] No in-memory forecast DF found for street '{street}'. Skipping plots.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    safe_city = str(meta['city']).replace(' ', '_').lower()\n",
    "    safe_street = str(street).replace(' ', '_').lower()\n",
    "\n",
    "    pred_file_path = str(img_dir / f\"{safe_city}_{safe_street}_pm25_forecast.png\")\n",
    "    hindcast_file_path = str(img_dir / f\"{safe_city}_{safe_street}_pm25_hindcast_1day.png\")\n",
    "\n",
    "\n",
    "    print(\n",
    "        street,\n",
    "        \"rows:\", len(nday_df),\n",
    "        \"nans:\", nday_df['predicted_pm25'].isna().sum() if 'predicted_pm25' in nday_df.columns else 'col-missing',\n",
    "        \"dates:\", nday_df['date'].min(), \"‚Üí\", nday_df['date'].max()\n",
    "    )\n",
    "    plt = util.plot_air_quality_forecast(meta['city'], street, nday_df, pred_file_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    if preds_1day.empty or outcomes.empty:\n",
    "        print(f\"[WARN] Empty preds_1day or outcomes for '{street}'. Skipping hindcast plot.\")\n",
    "    else:\n",
    "  \n",
    "        preds_1day['date'] = _norm_dates(preds_1day['date'])\n",
    "        outcomes['date']   = _norm_dates(outcomes['date'])\n",
    "\n",
    "        min_d, max_d = outcomes['date'].min(), outcomes['date'].max()\n",
    "        preds_1day = preds_1day[(preds_1day['date'] >= min_d) & (preds_1day['date'] <= max_d)].copy()\n",
    "\n",
    "        hindcast_df = (\n",
    "            preds_1day[['date', 'predicted_pm25']]\n",
    "            .merge(outcomes, on='date', how='inner')\n",
    "            .sort_values('date')\n",
    "        )\n",
    "\n",
    "        if hindcast_df.empty:\n",
    "            print(f\"[WARN] No overlapping dates for hindcast at '{street}'. Skipping hindcast plot.\")\n",
    "        else:\n",
    "            plt = util.plot_air_quality_forecast(\n",
    "                meta['city'], street, hindcast_df, hindcast=True, file_path=hindcast_file_path\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    hops_dir = f\"Resources/airquality/{safe_city}_{safe_street}_{today_str}\"\n",
    "    dataset_api.upload(pred_file_path, hops_dir, overwrite=True)\n",
    "    if Path(hindcast_file_path).exists():\n",
    "        dataset_api.upload(hindcast_file_path, hops_dir, overwrite=True)\n",
    "\n",
    "print(f\"The images are saved here: {project.get_url()}/settings/fb/path/Resources/airquality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb1552",
   "metadata": {},
   "source": [
    "### Plot the Hindcast comparing predicted with forecasted values (1-day prior forecast)\n",
    "\n",
    "__This graph will be empty to begin with - this is normal.__\n",
    "\n",
    "After a few days of predictions and observations, you will get data points in this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf88a81-5a3e-43d7-a8a5-177e5c2b5387",
   "metadata": {},
   "source": [
    "### Upload the prediction and hindcast dashboards (png files) to Hopsworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29eb549",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1d89e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
